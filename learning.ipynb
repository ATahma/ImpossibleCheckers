{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Board import Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9022325403738445"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 1 2 3 4 5 6 7\n",
      "\n",
      "0  |▓| |▓| |▓| |▓| |\n",
      "1  | |▓| |▓| |▓| |▓|\n",
      "2  |▓| |▓| |▓| |▓| |\n",
      "3  | | | | | | | | |\n",
      "4  | | | | | | | | |\n",
      "5  | |░| |░| |░| |░|\n",
      "6  |░| |░| |░| |░| |\n",
      "7  | |░| |░| |░| |░|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = Board()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 3)\n",
      "(2, 6)\n",
      "(0, 2)\n",
      "(2, 2)\n",
      "(0, 6)\n",
      "(1, 5)\n",
      "(2, 0)\n",
      "(2, 4)\n",
      "(0, 4)\n",
      "(1, 1)\n",
      "(1, 7)\n"
     ]
    }
   ],
   "source": [
    "for i in b.player1.pieces.values():\n",
    "    print(i.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing learning.py\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Board import Board\n",
    "\n",
    "class agent:\n",
    "    \"\"\"\n",
    "    Learning class used to implement the components for reinforcement learning\n",
    "    \"\"\"\n",
    "    # set the reward for each state\n",
    "    rwin = 0.0 # win reward\n",
    "    rloss = -1000.0\n",
    "    rtie = -50.0 # tie reward\n",
    "    rside = -1.0 # reward for each piece on the side of the board\n",
    "    rmiddle = -2.0 # reward for each piece in the middles\n",
    "    def __init__(self, expRate = 0, discount = 1):\n",
    "        \"\"\"\n",
    "        Initialize a Board object with the follwoing properties\n",
    "        states : all the past boards and the actions that led to them\n",
    "        expRate : between 0 and 1, probability of taking a random move\n",
    "        dc : the discount factor used to dampen the agent's rewards\n",
    "        current : the current state\n",
    "        creward : the reward for the current state\n",
    "        \"\"\"\n",
    "        self.board = Board()\n",
    "        # create a dictionary to keep track of all the past steps\n",
    "        self.states = {}\n",
    "        self.expRate = expRate\n",
    "        self.dc = discount\n",
    "        self.current = self.board.copy()\n",
    "    def domain(self, verbose = False, showBoard = True):\n",
    "        \"\"\"\n",
    "        return a tuple of lists, consisting of the possible moves for\n",
    "        respectively player 1 and player 2\n",
    "        also print the possible moves for display if \n",
    "        specified verbose = True (False by default)\n",
    "        print the board (unless specified otherwise)\n",
    "        \"\"\"\n",
    "        # print the moves if specified\n",
    "        # first get the legal moves for both players as sets\n",
    "        # this makes them more reusable for after\n",
    "        # also add any possible captures to the moves\n",
    "        self.p1legal = self.board.player1.legalMoves(self.board.player2)\n",
    "        self.p1legal.update(self.board.player1.legalCaptures(self.board.player2))\n",
    "        self.p2legal = self.board.player2.legalMoves(self.board.player1)\n",
    "        self.p2legal.update(self.board.player2.legalCaptures(self.board.player1))\n",
    "        if  verbose:\n",
    "            # go over all the specified legal moves for both players\n",
    "            print('PLAYER 1: {}'.format(self.board.player1.color()))\n",
    "            for move in self.p1legal:\n",
    "                print('{} --> {}'.format(move[0].pos,move[1]))      \n",
    "            print('PLAYER 2: {}'.format(self.board.player2.color()))\n",
    "            for move in self.p2legal:\n",
    "                print('{} --> {}'.format(move[0].pos,move[1]))\n",
    "        if showBoard: # print the current state of the board\n",
    "            print(self.board)\n",
    "        # now construct the possbile moves arrays\n",
    "        # this a numpy array object made of tuples wher\n",
    "        # tuple[0] is current position and tuple[1] is\n",
    "        # next psoition if the move is performed\n",
    "        self.p1moves = np.array([(move[0].pos,move[1]) for move in self.p1legal])\n",
    "        self.p2moves = np.array([(move[0].pos,move[1]) for move in self.p2legal])\n",
    "        return (self.p1moves.copy(), self.p2moves.copy())\n",
    "    @staticmethod\n",
    "    def reward(board, inclOpposite = False):\n",
    "        \"\"\"\n",
    "        Reward function that determines the reward \n",
    "        for the current state -- of player1\n",
    "        the input is a board object\n",
    "        \"\"\"\n",
    "        # check whether there is a winner\n",
    "        if board.winner() == board.player1:\n",
    "            return agent.rwin # this meand computer won\n",
    "        elif board.winner() == board.player2:\n",
    "            return agent.rloss # computer lost\n",
    "        # the other state would be when a winner is not decided yet or we have a tie\n",
    "        result = 0.0\n",
    "        for piece in board.player1.pieces.values():\n",
    "            # iterate over the dictionary of board pieces of player1\n",
    "            # basically, if a piece is on the side which is a more strategic\n",
    "            # point, give a reward\n",
    "            if piece.pos[1] == 0 or piece.pos[1] == 0: # if any piece is on the side\n",
    "                result += agent.rmiddle\n",
    "            else:\n",
    "                result += agent.rside\n",
    "        # I thought maybe I could do the same thing for the opposition board,\n",
    "        # each opposite side piece has -2.0 and middle ones have -1.0\n",
    "        # I will keep this optional for testing\n",
    "        if inclOpposite:\n",
    "            for piece in board.player1.pieces.values():\n",
    "                # iterate over the dictionary of board pieces of player2\n",
    "                if piece.pos[1] == 0 or piece.pos[1] == 0: # if any piece is on the side\n",
    "                    result += agent.rside\n",
    "                else:\n",
    "                    result += agent.rmiddle\n",
    "        return result\n",
    "    def policy(self):\n",
    "        \"\"\"\n",
    "        Method that chooses the best next move\n",
    "        \"\"\"\n",
    "        # using the experimentation rate, randomize the next move (or not)\n",
    "        # use np.random.random that return numbers in [0,1) from a continuous distribution\n",
    "        # find all the possible legal moves and captures\n",
    "        legalMoves = self.board.player1.legalMoves(self.board.player2)\n",
    "        legalMoves.update(self.board.player1.legalCaptures(self.board.player2))\n",
    "        if np.random.random() < self.expRate:\n",
    "            print(legalMoves)\n",
    "    def saveState(self):\n",
    "        \"\"\"Save the state of the current board and the action\n",
    "            leading to it inside the self.states dictionary\n",
    "        \"\"\"\n",
    "        self.states.append()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = agent(expRate=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(Piece((2, 6), white), (3, 5)), (Piece((2, 0), white), (3, 1)), (Piece((2, 6), white), (3, 7)), (Piece((2, 2), white), (3, 3)), (Piece((2, 4), white), (3, 5)), (Piece((2, 4), white), (3, 3)), (Piece((2, 2), white), (3, 1))}\n"
     ]
    }
   ],
   "source": [
    "l.policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAYER 1: white\n",
      "(2, 2) --> (3, 1)\n",
      "(2, 2) --> (3, 3)\n",
      "(2, 0) --> (3, 1)\n",
      "(2, 4) --> (3, 5)\n",
      "(2, 4) --> (3, 3)\n",
      "(2, 6) --> (3, 7)\n",
      "(2, 6) --> (3, 5)\n",
      "PLAYER 2: black\n",
      "(5, 3) --> (4, 2)\n",
      "(5, 5) --> (4, 4)\n",
      "(5, 5) --> (4, 6)\n",
      "(5, 7) --> (4, 6)\n",
      "(5, 1) --> (4, 0)\n",
      "(5, 3) --> (4, 4)\n",
      "(5, 1) --> (4, 2)\n",
      "    0 1 2 3 4 5 6 7\n",
      "\n",
      "0  |▓| |▓| |▓| |▓| |\n",
      "1  | |▓| |▓| |▓| |▓|\n",
      "2  |▓| |▓| |▓| |▓| |\n",
      "3  | | | | | | | | |\n",
      "4  | | | | | | | | |\n",
      "5  | |░| |░| |░| |░|\n",
      "6  |░| |░| |░| |░| |\n",
      "7  | |░| |░| |░| |░|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(a,b) = l.domain(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(set(list(range(1,100,4))).update())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(list(range(40,60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1,\n",
       " 5,\n",
       " 9,\n",
       " 13,\n",
       " 17,\n",
       " 21,\n",
       " 25,\n",
       " 29,\n",
       " 33,\n",
       " 37,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 61,\n",
       " 65,\n",
       " 69,\n",
       " 73,\n",
       " 77,\n",
       " 81,\n",
       " 85,\n",
       " 89,\n",
       " 93,\n",
       " 97}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.update(list(range(1,100,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
